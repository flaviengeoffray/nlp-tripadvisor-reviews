model_path: "configurations/Rnn_Classification/"

# tokenizer:
#   type: "bpe"
#   # checkpoint: "configurations/Rnn_Classification/tokenizer.json"
#   params:
#     vocab_size: 10000
    # min_frequency: 2

augmented_data: "augmented.csv"
# sample_size: 100
vectorizer:
  type: "word2vec"
  # is_embedding: true
  params:
    vector_size: 10
    max_len: 32
  #   stop_words: "english"
  #   norm: "l2"
  #   use_idf: true
  #   smooth_idf: true
# vectorizer:
#   type: "word2vec"
#   checkpoint: null  
#   params:
#     vector_size: 1000
#     window: 5
#     min_count: 1
#     workers: 4
#     epochs: 5
#     topn: 10 


model:
  type: "rnn-classification"
  # checkpoint: "configurations/Rnn_Classification/best_model.pt"
  params:
    vocab_size: 10000
    input_dim: 10
    hidden_size: 256
    num_layers: 1
    output_dim: 5
    dropout_rate: 0.2
    lr: 0.0001
    batch_size: 32
    epochs: 500
    # patience: 5
    scheduler: true 
    device: "cuda"